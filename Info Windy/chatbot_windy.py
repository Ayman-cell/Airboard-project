#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Chatbot adapt√© pour Windy Safi - Utilise les donn√©es GP2 et Open Meteo
Architecture RAG bas√©e sur llama.py avec hybrid retrieval
"""

import os
import json
import re
import time
import uuid
import hashlib
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
from dataclasses import dataclass
import logging

import numpy as np
import requests
from langchain_core.documents import Document as LangchainDocument
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from sentence_transformers import CrossEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# Initialiser le logger AVANT de l'utiliser
logger = logging.getLogger(__name__)

# Charger les variables d'environnement depuis le fichier .env
try:
    from dotenv import load_dotenv
    # Charger le fichier .env depuis le r√©pertoire du script
    script_dir = Path(__file__).parent
    env_path = script_dir / '.env'
    if env_path.exists():
        load_dotenv(env_path)
    else:
        # Essayer aussi dans le r√©pertoire parent
        parent_env_path = script_dir.parent / '.env'
        if parent_env_path.exists():
            load_dotenv(parent_env_path)
        else:
            # Essayer dans le r√©pertoire de travail courant
            load_dotenv()
except ImportError:
    # python-dotenv n'est pas install√©, continuer sans
    pass

# Configuration de l'API Cerebras
# La cl√© API doit √™tre configur√©e via variable d'environnement
# Utilisez setup_env.py pour configurer votre cl√© ou cr√©ez un fichier .env
# Mode debug : le serveur peut d√©marrer sans cl√© API (fonctionnalit√©s chatbot d√©sactiv√©es)
CEREBRAS_API_KEY = os.environ.get("CEREBRAS_API_KEY")
CEREBRAS_AVAILABLE_FOR_CHATBOT = False

if not CEREBRAS_API_KEY:
    logger.warning("‚ö†Ô∏è  Cl√© API Cerebras manquante - Mode debug activ√©")
    logger.info("üí° Le serveur peut d√©marrer, mais les fonctionnalit√©s chatbot seront d√©sactiv√©es")
    logger.info("üìù Pour activer le chatbot, configurez votre cl√© API avec: python setup_env.py")
    logger.info("üìñ Ou cr√©ez un fichier .env avec: CEREBRAS_API_KEY=votre_cle_ici")
else:
    os.environ["CEREBRAS_API_KEY"] = CEREBRAS_API_KEY
    CEREBRAS_AVAILABLE_FOR_CHATBOT = True

# Dossier pour sauvegarder les conversations
CONVERSATIONS_DIR = "conversations_weather"
if not os.path.exists(CONVERSATIONS_DIR):
    os.makedirs(CONVERSATIONS_DIR)

try:
    from langchain_cerebras import ChatCerebras
    LANGCHAIN_CEREBRAS_AVAILABLE = True
except ImportError:
    logger.warning("langchain_cerebras non disponible, utilisation d'un mode d√©grad√©")
    LANGCHAIN_CEREBRAS_AVAILABLE = False

# Disponibilit√© globale de Cerebras (n√©cessite √† la fois la cl√© API et le module)
CEREBRAS_AVAILABLE = CEREBRAS_AVAILABLE_FOR_CHATBOT and LANGCHAIN_CEREBRAS_AVAILABLE

# Configuration des limites de tokens pour Llama 3.1-8B sur Cerebras (identique √† llama.py)
TOKEN_LIMITS = {
    "max_requests_per_minute": 30,
    "max_tokens_per_minute": 64000,
    "max_requests_per_hour": 900,
    "max_tokens_per_hour": 1000000,
    "max_requests_per_day": 14400,
    "max_tokens_per_day": 1000000,
    "max_tokens_per_request": 8000,
    "chunk_size": 4000,
    "delay_between_requests": 2,
    "max_retries": 3
}

# Configuration RAG (identique √† llama.py)
RAG_CONFIG = {
    "general": {
        "chunk_size": 1200,
        "chunk_overlap": 300,
        "separators": ["\n\n", "\n", " ", ""]
    }
}

# Mod√®le de re-ranking (cross-encoder)
RERANKER_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"


@dataclass
class UsageStats:
    """Statistiques d'utilisation Cerebras (identique √† llama.py)"""
    requests_this_minute: int = 0
    tokens_this_minute: int = 0
    requests_this_hour: int = 0
    tokens_this_hour: int = 0
    requests_this_day: int = 0
    tokens_this_day: int = 0
    last_minute_reset: float = 0
    last_hour_reset: float = 0
    last_day_reset: float = 0
    request_times: List[float] = None
    
    def __post_init__(self):
        if self.request_times is None:
            self.request_times = []


class CerebrasTokenManager:
    """Gestionnaire de tokens Cerebras (identique √† llama.py)"""
    
    def __init__(self):
        self.stats = UsageStats()
        self.stats.last_minute_reset = time.time()
        self.stats.last_hour_reset = time.time()
        self.stats.last_day_reset = time.time()
        
    def count_tokens(self, text: str) -> int:
        return len(text) // 3
    
    def reset_counters_if_needed(self):
        current_time = time.time()
        
        if current_time - self.stats.last_minute_reset >= 60:
            self.stats.requests_this_minute = 0
            self.stats.tokens_this_minute = 0
            self.stats.last_minute_reset = current_time
            self.stats.request_times = [t for t in self.stats.request_times if current_time - t < 60]
        
        if current_time - self.stats.last_hour_reset >= 3600:
            self.stats.requests_this_hour = 0
            self.stats.tokens_this_hour = 0
            self.stats.last_hour_reset = current_time
        
        if current_time - self.stats.last_day_reset >= 86400:
            self.stats.requests_this_day = 0
            self.stats.tokens_this_day = 0
            self.stats.last_day_reset = current_time
    
    def can_make_request(self, estimated_tokens: int) -> Dict[str, Any]:
        self.reset_counters_if_needed()
        
        checks = {
            "can_proceed": True,
            "blocking_limit": None,
            "wait_time": 0,
            "details": {}
        }
        
        if (self.stats.requests_this_minute + 1) > TOKEN_LIMITS["max_requests_per_minute"]:
            checks["can_proceed"] = False
            checks["blocking_limit"] = "requests_per_minute"
            checks["wait_time"] = 60 - (time.time() - self.stats.last_minute_reset)
        elif (self.stats.tokens_this_minute + estimated_tokens) > TOKEN_LIMITS["max_tokens_per_minute"]:
            checks["can_proceed"] = False
            checks["blocking_limit"] = "tokens_per_minute"
            checks["wait_time"] = 60 - (time.time() - self.stats.last_minute_reset)
        elif (self.stats.requests_this_hour + 1) > TOKEN_LIMITS["max_requests_per_hour"]:
            checks["can_proceed"] = False
            checks["blocking_limit"] = "requests_per_hour"
            checks["wait_time"] = 3600 - (time.time() - self.stats.last_hour_reset)
        elif (self.stats.tokens_this_hour + estimated_tokens) > TOKEN_LIMITS["max_tokens_per_hour"]:
            checks["can_proceed"] = False
            checks["blocking_limit"] = "tokens_per_hour"
            checks["wait_time"] = 3600 - (time.time() - self.stats.last_hour_reset)
        elif (self.stats.requests_this_day + 1) > TOKEN_LIMITS["max_requests_per_day"]:
            checks["can_proceed"] = False
            checks["blocking_limit"] = "requests_per_day"
            checks["wait_time"] = 86400 - (time.time() - self.stats.last_day_reset)
        elif (self.stats.tokens_this_day + estimated_tokens) > TOKEN_LIMITS["max_tokens_per_day"]:
            checks["can_proceed"] = False
            checks["blocking_limit"] = "tokens_per_day"
            checks["wait_time"] = 86400 - (time.time() - self.stats.last_day_reset)
        
        checks["details"] = {
            "current_minute": {
                "requests": self.stats.requests_this_minute,
                "tokens": self.stats.tokens_this_minute
            },
            "current_hour": {
                "requests": self.stats.requests_this_hour,
                "tokens": self.stats.tokens_this_hour
            },
            "current_day": {
                "requests": self.stats.requests_this_day,
                "tokens": self.stats.tokens_this_day
            }
        }
        
        return checks
    
    def wait_if_needed(self, estimated_tokens: int):
        """Attend si n√©cessaire pour respecter les limites (sans Streamlit)"""
        check_result = self.can_make_request(estimated_tokens)
        
        if not check_result["can_proceed"]:
            wait_time = check_result["wait_time"]
            limit_type = check_result["blocking_limit"]
            
            limit_names = {
                "requests_per_minute": "requ√™tes par minute",
                "tokens_per_minute": "tokens par minute",
                "requests_per_hour": "requ√™tes par heure",
                "tokens_per_hour": "tokens par heure",
                "requests_per_day": "requ√™tes par jour",
                "tokens_per_day": "tokens par jour"
            }
            
            logger.warning(f"‚è≥ Limite Cerebras atteinte: {limit_names.get(limit_type, limit_type)}")
            logger.info(f"‚è∞ Attente de {wait_time:.1f} secondes...")
            time.sleep(wait_time)
            self.reset_counters_if_needed()
    
    def record_request(self, tokens_used: int):
        current_time = time.time()
        
        self.stats.requests_this_minute += 1
        self.stats.requests_this_hour += 1
        self.stats.requests_this_day += 1
        
        self.stats.tokens_this_minute += tokens_used
        self.stats.tokens_this_hour += tokens_used
        self.stats.tokens_this_day += tokens_used
        
        self.stats.request_times.append(current_time)
    
    def get_usage_summary(self) -> Dict[str, Any]:
        """Retourne un r√©sum√© de l'utilisation (identique √† llama.py)"""
        self.reset_counters_if_needed()
        
        return {
            "minute": {
                "requests": f"{self.stats.requests_this_minute}/{TOKEN_LIMITS['max_requests_per_minute']}",
                "tokens": f"{self.stats.tokens_this_minute:,}/{TOKEN_LIMITS['max_tokens_per_minute']:,}",
                "requests_percent": (self.stats.requests_this_minute / TOKEN_LIMITS['max_requests_per_minute']) * 100,
                "tokens_percent": (self.stats.tokens_this_minute / TOKEN_LIMITS['max_tokens_per_minute']) * 100
            },
            "hour": {
                "requests": f"{self.stats.requests_this_hour}/{TOKEN_LIMITS['max_requests_per_hour']}",
                "tokens": f"{self.stats.tokens_this_hour:,}/{TOKEN_LIMITS['max_tokens_per_hour']:,}",
                "requests_percent": (self.stats.requests_this_hour / TOKEN_LIMITS['max_requests_per_hour']) * 100,
                "tokens_percent": (self.stats.tokens_this_hour / TOKEN_LIMITS['max_tokens_per_hour']) * 100
            },
            "day": {
                "requests": f"{self.stats.requests_this_day}/{TOKEN_LIMITS['max_requests_per_day']}",
                "tokens": f"{self.stats.tokens_this_day:,}/{TOKEN_LIMITS['max_tokens_per_day']:,}",
                "requests_percent": (self.stats.requests_this_day / TOKEN_LIMITS['max_requests_per_day']) * 100,
                "tokens_percent": (self.stats.tokens_this_day / TOKEN_LIMITS['max_tokens_per_day']) * 100
            }
        }


# Instance globale du gestionnaire de tokens Cerebras
cerebras_token_manager = CerebrasTokenManager()


def safe_cerebras_call(llm, prompt: str, max_retries: int = TOKEN_LIMITS["max_retries"]) -> str:
    """Appel s√©curis√© au LLM Cerebras avec gestion compl√®te des erreurs (identique √† llama.py)"""
    estimated_tokens = cerebras_token_manager.count_tokens(prompt)
    cerebras_token_manager.wait_if_needed(estimated_tokens)
    
    for attempt in range(max_retries):
        try:
            if attempt > 0:
                time.sleep(TOKEN_LIMITS["delay_between_requests"])
            
            response = llm.invoke(prompt)
            
            if hasattr(response, 'content'):
                response_text = response.content
            else:
                response_text = str(response)
            
            response_tokens = cerebras_token_manager.count_tokens(response_text)
            cerebras_token_manager.record_request(estimated_tokens + response_tokens)
            
            return response_text
            
        except Exception as e:
            error_msg = str(e).lower()
            if "rate limit" in error_msg or "too many requests" in error_msg:
                wait_time = (attempt + 1) * 30
                logger.warning(f"‚ö† Limite Cerebras atteinte. Attente de {wait_time} secondes... (Tentative {attempt + 1}/{max_retries})")
                time.sleep(wait_time)
                continue
            elif "token" in error_msg and "limit" in error_msg:
                logger.error("‚ùå Limite de tokens d√©pass√©e. Essayez avec un texte plus court.")
                break
            else:
                if attempt == max_retries - 1:
                    logger.error(f"‚ùå Erreur Cerebras apr√®s {max_retries} tentatives: {str(e)}")
                    break
                else:
                    logger.warning(f"‚ö† Erreur Cerebras (tentative {attempt + 1}/{max_retries}): {str(e)}")
                    time.sleep(5)
    
    return "D√©sol√©, je n'ai pas pu traiter votre demande en raison de limitations techniques avec Cerebras."


def detect_language_simple(text: str) -> str:
    """D√©tecte la langue d'un texte de mani√®re simple (sans Cerebras)"""
    if not text.strip():
        return "fr"
    
    text_lower = text.lower()
    if any(word in text_lower for word in ["the", "and", "is", "are", "you", "your"]):
        return "en"
    elif any(word in text_lower for word in ["el", "la", "es", "son", "tu", "su"]):
        return "es"
    elif any(word in text_lower for word in ["der", "die", "das", "ist", "sind", "ihr"]):
        return "de"
    elif any(word in text_lower for word in ["il", "la", "√®", "sono", "tu", "tuo"]):
        return "it"
    elif any(word in text_lower for word in ["ŸÅŸä", "ŸÖŸÜ", "ÿ•ŸÑŸâ", "Ÿáÿ∞ÿß", "ÿßŸÑÿ™Ÿä"]):
        return "ar"
    else:
        return "fr"


def generate_smart_title(first_message: str) -> str:
    """G√©n√®re un titre intelligent pour la conversation (identique √† llama.py)"""
    try:
        title_prompt = f"""Tu es un expert en cr√©ation de titres courts et descriptifs pour des conversations de chat m√©t√©orologique.

T√ÇCHE: Cr√©er un titre court (3-6 mots maximum) qui r√©sume parfaitement le sujet de cette question.

QUESTION DE L'UTILISATEUR: {first_message[:300]}

R√àGLES STRICTES:
1. Maximum 6 mots
2. Pas de guillemets, apostrophes, ou caract√®res sp√©ciaux, pas de hashtag
3. Pas de ponctuation √† la fin
4. Utilise des mots simples et clairs
5. Commence par un verbe d'action ou un nom
6. √âvite les articles (le, la, les, un, une)

EXEMPLES DE BONS TITRES:
- Question: "Quelle est la temp√©rature moyenne cette semaine ?" ‚Üí Titre: "Temp√©rature moyenne semaine"
- Question: "Quel est le vent √† GP2 ?" ‚Üí Titre: "Vent station GP2"
- Question: "Pr√©visions m√©t√©o demain" ‚Üí Titre: "Pr√©visions m√©t√©o demain"

R√âPONSE (seulement le titre, rien d'autre):"""

        if CEREBRAS_AVAILABLE:
            try:
                llm = ChatCerebras(
                    model="llama3.1-8b",
                    temperature=0.1,
                    max_tokens=50
                )
                response = safe_cerebras_call(llm, title_prompt)
                title = response.strip()
            except:
                title = ""
        else:
            title = ""
        
        # Nettoyer le titre
        title = title.replace('"', '').replace("'", '').replace('`', '')
        title = title.replace('¬´', '').replace('¬ª', '').replace('"', '').replace('"', '')
        title = title.replace(':', '').replace(';', '').replace('!', '').replace('?', '')
        title = title.replace('\n', ' ').replace('\r', ' ').replace('#', '')
        title = ' '.join(title.split())
        
        if len(title) > 40:
            words = title.split()
            title = ' '.join(words[:5])
        
        if not title or len(title) < 3:
            words = first_message.split()[:10]
            important_words = [w for w in words if len(w) > 3 and w.lower() not in ['comment', 'puis', 'peux', 'veux', 'faire', 'avec', 'pour', 'dans', 'cette', 'quelle', 'quel']]
            title = ' '.join(important_words[:3]) if important_words else first_message[:25]
        
        return title.strip()
    except Exception as e:
        logger.error(f"Erreur lors de la g√©n√©ration du titre: {e}")
        words = first_message.split()[:5]
        return ' '.join(words).replace('?', '').replace('!', '')[:30]


def save_conversation(messages: List[Dict], title: Optional[str] = None) -> Optional[str]:
    """Sauvegarde une conversation dans un fichier JSON (adapt√© de llama.py)"""
    try:
        if not messages or len(messages) == 0:
            logger.warning("‚ùå Aucun message √† sauvegarder")
            return None
        
        user_messages = [msg for msg in messages if msg.get("role") == "user"]
        if len(user_messages) == 0:
            logger.warning("‚ùå Aucun message utilisateur trouv√©")
            return None
        
        try:
            if not os.path.exists(CONVERSATIONS_DIR):
                os.makedirs(CONVERSATIONS_DIR)
        except Exception as e:
            logger.error(f"‚ùå Impossible de cr√©er le dossier: {e}")
            return None
        
        conversation_id = str(uuid.uuid4())
        timestamp = datetime.now()
        
        if not title:
            first_user_message = next((msg.get("content", "") for msg in messages if msg.get("role") == "user"), "")
            if first_user_message:
                title = generate_smart_title(first_user_message)
            else:
                title = "Conversation m√©t√©o"
        
        conversation_data = {
            "id": conversation_id,
            "title": title,
            "mode": "weather_chat",
            "timestamp": timestamp.isoformat(),
            "messages": messages,
            "message_count": len(messages)
        }
        
        filename = f"{CONVERSATIONS_DIR}/conv_{conversation_id}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(conversation_data, f, ensure_ascii=False, indent=2)
            
            if os.path.exists(filename):
                logger.info(f"‚úÖ Conversation sauvegard√©e: {title}")
                return conversation_id
            else:
                logger.error("‚ùå Le fichier n'a pas √©t√© cr√©√©")
                return None
        except PermissionError:
            logger.error("‚ùå Erreur de permissions - impossible d'√©crire le fichier")
            return None
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'√©criture: {e}")
            return None
    except Exception as e:
        logger.error(f"‚ùå Erreur g√©n√©rale lors de la sauvegarde: {e}")
        return None


def load_conversations() -> List[Dict]:
    """Charge toutes les conversations sauvegard√©es"""
    conversations = []
    if not os.path.exists(CONVERSATIONS_DIR):
        return conversations
    
    for filename in os.listdir(CONVERSATIONS_DIR):
        if filename.startswith("conv_") and filename.endswith(".json"):
            try:
                with open(f"{CONVERSATIONS_DIR}/{filename}", 'r', encoding='utf-8') as f:
                    conv_data = json.load(f)
                conversations.append(conv_data)
            except Exception as e:
                logger.error(f"Erreur lors du chargement de {filename}: {e}")
    
    conversations.sort(key=lambda x: x.get("timestamp", ""), reverse=True)
    return conversations


def load_conversation(conversation_id: str) -> Optional[Dict]:
    """Charge une conversation sp√©cifique"""
    filename = f"{CONVERSATIONS_DIR}/conv_{conversation_id}.json"
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception as e:
        logger.error(f"Erreur lors du chargement de la conversation: {e}")
        return None


def delete_conversation(conversation_id: str) -> bool:
    """Supprime une conversation"""
    filename = f"{CONVERSATIONS_DIR}/conv_{conversation_id}.json"
    try:
        os.remove(filename)
        return True
    except Exception as e:
        logger.error(f"Erreur lors de la suppression: {e}")
        return False


def update_existing_conversation(conversation_id: str, messages: List[Dict]) -> Optional[str]:
    """Met √† jour une conversation existante avec les nouveaux messages"""
    filename = f"{CONVERSATIONS_DIR}/conv_{conversation_id}.json"
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            conv_data = json.load(f)
        
        conv_data['messages'] = messages
        conv_data['message_count'] = len(messages)
        conv_data['timestamp'] = datetime.now().isoformat()
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(conv_data, f, ensure_ascii=False, indent=2)
        
        return conversation_id
    except Exception as e:
        logger.error(f"Erreur lors de la mise √† jour: {e}")
        return save_conversation(messages)


def get_location_coordinates(location_name: str) -> Optional[Dict[str, Any]]:
    """Retourne les coordonn√©es (lat, lon) pour un nom de lieu/pays"""
    # Mapping de pays/r√©gions vers leurs coordonn√©es repr√©sentatives
    location_map = {
        # France
        "france": {"lat": 46.2276, "lon": 2.2137, "name": "France (centre)"},
        "paris": {"lat": 48.8566, "lon": 2.3522, "name": "Paris, France"},
        "lyon": {"lat": 45.7640, "lon": 4.8357, "name": "Lyon, France"},
        "marseille": {"lat": 43.2965, "lon": 5.3698, "name": "Marseille, France"},
        "toulouse": {"lat": 43.6047, "lon": 1.4442, "name": "Toulouse, France"},
        "nice": {"lat": 43.7102, "lon": 7.2620, "name": "Nice, France"},
        "bordeaux": {"lat": 44.8378, "lon": -0.5792, "name": "Bordeaux, France"},
        
        # Espagne
        "espagne": {"lat": 40.4637, "lon": -3.7492, "name": "Espagne (centre)"},
        "spain": {"lat": 40.4637, "lon": -3.7492, "name": "Espagne (centre)"},
        "madrid": {"lat": 40.4168, "lon": -3.7038, "name": "Madrid, Espagne"},
        "barcelone": {"lat": 41.3851, "lon": 2.1734, "name": "Barcelone, Espagne"},
        "barcelona": {"lat": 41.3851, "lon": 2.1734, "name": "Barcelone, Espagne"},
        
        # Italie
        "italie": {"lat": 41.8719, "lon": 12.5674, "name": "Italie (centre)"},
        "italy": {"lat": 41.8719, "lon": 12.5674, "name": "Italie (centre)"},
        "rome": {"lat": 41.9028, "lon": 12.4964, "name": "Rome, Italie"},
        "milan": {"lat": 45.4642, "lon": 9.1900, "name": "Milan, Italie"},
        "naples": {"lat": 40.8518, "lon": 14.2681, "name": "Naples, Italie"},
        
        # Allemagne
        "allemagne": {"lat": 51.1657, "lon": 10.4515, "name": "Allemagne (centre)"},
        "germany": {"lat": 51.1657, "lon": 10.4515, "name": "Allemagne (centre)"},
        "berlin": {"lat": 52.5200, "lon": 13.4050, "name": "Berlin, Allemagne"},
        "munich": {"lat": 48.1351, "lon": 11.5820, "name": "Munich, Allemagne"},
        "hamburg": {"lat": 53.5511, "lon": 9.9937, "name": "Hamburg, Allemagne"},
        
        # Royaume-Uni
        "angleterre": {"lat": 52.3555, "lon": -1.1743, "name": "Angleterre (centre)"},
        "england": {"lat": 52.3555, "lon": -1.1743, "name": "Angleterre (centre)"},
        "londres": {"lat": 51.5074, "lon": -0.1278, "name": "Londres, Royaume-Uni"},
        "london": {"lat": 51.5074, "lon": -0.1278, "name": "Londres, Royaume-Uni"},
        "royaume-uni": {"lat": 51.5074, "lon": -0.1278, "name": "Royaume-Uni"},
        "uk": {"lat": 51.5074, "lon": -0.1278, "name": "Royaume-Uni"},
        
        # Belgique
        "belgique": {"lat": 50.5039, "lon": 4.4699, "name": "Belgique (centre)"},
        "belgium": {"lat": 50.5039, "lon": 4.4699, "name": "Belgique (centre)"},
        "bruxelles": {"lat": 50.8503, "lon": 4.3517, "name": "Bruxelles, Belgique"},
        "brussels": {"lat": 50.8503, "lon": 4.3517, "name": "Bruxelles, Belgique"},
        
        # Suisse
        "suisse": {"lat": 46.8182, "lon": 8.2275, "name": "Suisse (centre)"},
        "switzerland": {"lat": 46.8182, "lon": 8.2275, "name": "Suisse (centre)"},
        "zurich": {"lat": 47.3769, "lon": 8.5417, "name": "Zurich, Suisse"},
        "gen√®ve": {"lat": 46.2044, "lon": 6.1432, "name": "Gen√®ve, Suisse"},
        "geneva": {"lat": 46.2044, "lon": 6.1432, "name": "Gen√®ve, Suisse"},
        
        # Portugal
        "portugal": {"lat": 39.3999, "lon": -8.2245, "name": "Portugal (centre)"},
        "lisbonne": {"lat": 38.7223, "lon": -9.1393, "name": "Lisbonne, Portugal"},
        "lisbon": {"lat": 38.7223, "lon": -9.1393, "name": "Lisbonne, Portugal"},
    }
    
    location_lower = location_name.lower().strip()
    return location_map.get(location_lower)


def fetch_global_weather_data(lat: float, lon: float, location_name: str, model: str = "auto") -> Optional[Dict]:
    """R√©cup√®re les donn√©es m√©t√©orologiques globales via Open-Meteo pour une coordonn√©e"""
    try:
        url = "https://api.open-meteo.com/v1/forecast"
        params = {
            "latitude": lat,
            "longitude": lon,
            "hourly": "temperature_2m,relative_humidity_2m,wind_speed_10m,wind_direction_10m",
            "wind_speed_unit": "ms",
            "forecast_days": 7,
            "timezone": "UTC",
        }
        
        if model and model != "auto":
            params["models"] = model
        
        logger.info(f"R√©cup√©ration donn√©es globales Open-Meteo pour {location_name} ({lat}, {lon})")
        
        try:
            resp = requests.get(url, params=params, timeout=15)
            resp.raise_for_status()
        except requests.exceptions.Timeout:
            logger.warning(f"Timeout lors de la r√©cup√©ration des donn√©es globales pour {location_name}")
            return None
        except requests.exceptions.ConnectionError as e:
            logger.warning(f"Erreur de connexion lors de la r√©cup√©ration des donn√©es globales pour {location_name}: {e}")
            return None
        except requests.exceptions.RequestException as e:
            logger.warning(f"Erreur de requ√™te lors de la r√©cup√©ration des donn√©es globales pour {location_name}: {e}")
            return None
        
        try:
            data = resp.json()
        except ValueError as e:
            logger.warning(f"Erreur de parsing JSON pour {location_name}: {e}")
            return None
        
        hourly = data.get("hourly", {})
        temps = hourly.get("temperature_2m", [])
        times = hourly.get("time", [])
        
        if not temps or not times:
            logger.warning(f"Aucune donn√©e de temp√©rature trouv√©e pour {location_name}")
            return None
        
        # Calculer les statistiques
        temps_valid = [t for t in temps if t is not None]
        if not temps_valid:
            logger.warning(f"Aucune temp√©rature valide trouv√©e pour {location_name}")
            return None
        
        return {
            "location": location_name,
            "lat": lat,
            "lon": lon,
            "model": model,
            "times": times,
            "temperatures": temps,
            "stats": {
                "mean": float(np.mean(temps_valid)),
                "min": float(np.min(temps_valid)),
                "max": float(np.max(temps_valid)),
                "count": len(temps_valid)
            },
            "available": True
        }
    except Exception as e:
        logger.error(f"Erreur inattendue lors de la r√©cup√©ration des donn√©es globales pour {location_name}: {e}", exc_info=True)
        return None


def is_global_question(question: str) -> bool:
    """D√©tecte si la question est une question globale/g√©n√©rale qui n√©cessite des donn√©es mondiales"""
    question_lower = question.lower()
    
    # Mots-cl√©s indiquant explicitement une question globale/mondiale
    global_keywords_explicit = [
        "du globe", "du monde", "mondiale", "globale", "plan√©taire",
        "partout", "tous les pays", "tous les continents", "toute la plan√®te",
        "climat mondial", "m√©t√©o mondiale", "conditions mondiales",
        "temp√©rature mondiale", "temp√©rature globale", "temp√©rature plan√©taire"
    ]
    
    # Si la question contient explicitement des mots indiquant le globe/monde, c'est global
    if any(keyword in question_lower for keyword in global_keywords_explicit):
        return True
    
    # Sinon, si la question mentionne "zone la plus chaude" ou similaire SANS mentionner "du globe",
    # c'est probablement sur la zone visualis√©e (pas global)
    if any(phrase in question_lower for phrase in [
        "zone la plus chaude", "zone la plus froide", "endroit le plus chaud", "endroit le plus froid"
    ]):
        # Si pas de mention explicite du globe/monde, consid√©rer comme question locale
        return False
    
    return False


def detect_and_fetch_global_data(question: str) -> Optional[Dict]:
    """D√©tecte si la question concerne un autre pays/r√©gion et r√©cup√®re les donn√©es si n√©cessaire"""
    # D√âSACTIV√â : Les appels directs √† Open-Meteo entrent en conflit avec le calcul du champ corrig√©
    # Les donn√©es globales doivent √™tre r√©cup√©r√©es via le serveur principal, pas directement ici
    # pour √©viter les conflits avec le processus de fusion (Helmholtz) qui utilise aussi Open-Meteo
    logger.info("R√©cup√©ration des donn√©es globales d√©sactiv√©e pour √©viter les conflits avec le champ corrig√©")
    return None


def detect_geographic_mismatch(question: str, station_data: Dict, forecast_data: Optional[Dict] = None) -> Optional[str]:
    """D√©tecte si la question concerne une zone g√©ographique diff√©rente de celle couverte par les donn√©es"""
    question_lower = question.lower()
    
    # Pays et r√©gions couverts par les donn√©es (autour de GP2 au Maroc)
    covered_regions = {
        "maroc", "morocco", "safi", "casablanca", "rabat", "marrakech",
        "afrique du nord", "north africa", "maghreb"
    }
    
    # Pays/r√©gions souvent demand√©s mais non couverts
    other_regions = {
        "france", "paris", "lyon", "marseille", "toulouse", "nice", "bordeaux",
        "espagne", "spain", "madrid", "barcelone", "barcelona",
        "italie", "italy", "rome", "milan", "naples",
        "allemagne", "germany", "berlin", "munich", "hamburg",
        "angleterre", "england", "londres", "london", "royaume-uni", "uk",
        "belgique", "belgium", "bruxelles", "brussels",
        "suisse", "switzerland", "zurich", "gen√®ve", "geneva",
        "portugal", "lisbonne", "lisbon",
        "europe", "european", "europ√©en"
    }
    
    # V√©rifier si la question mentionne une r√©gion non couverte
    for region in other_regions:
        if region in question_lower:
            # Extraire la zone couverte par les donn√©es
            covered_area = "la r√©gion de Safi (Maroc)"
            if station_data and "station" in station_data:
                st = station_data.get("station", {})
                if st.get("name"):
                    covered_area = f"la station {st.get('name')} (Maroc)"
            
            return f"Les donn√©es disponibles concernent uniquement {covered_area} (position: {station_data.get('station', {}).get('lat', 'N/A')}¬∞N, {station_data.get('station', {}).get('lon', 'N/A')}¬∞E). Je ne peux pas fournir d'informations m√©t√©orologiques pour {region.capitalize()} car ces donn√©es ne sont pas disponibles dans le contexte fourni."
    
    return None


class WeatherRAGProcessor:
    """Processeur RAG pour les donn√©es m√©t√©orologiques - bas√© sur AdvancedRAGProcessor de llama.py"""
    
    @staticmethod
    def create_weather_documents(station_data: Dict, forecast_data: Optional[Dict] = None, global_data: Optional[Dict] = None) -> List[LangchainDocument]:
        """Cr√©e des documents Langchain √† partir des donn√©es m√©t√©o (locale + globale si disponible)"""
        documents = []
        
        # Document 1: Donn√©es station GP2 temps r√©el
        if station_data and "station_data" in station_data:
            sd = station_data["station_data"]
            st = station_data.get("station", {})
            
            station_text = f"""DONN√âES STATION GP2 - TEMPS R√âEL

‚ö†Ô∏è IMPORTANT - ZONE G√âOGRAPHIQUE :
Ces donn√©es concernent UNIQUEMENT la station GP2 situ√©e au Maroc (r√©gion de Safi).
Cette station ne fournit PAS de donn√©es pour d'autres pays ou r√©gions.

Station: {st.get('name', 'GP2')}
Position: {st.get('lat', 'N/A')}¬∞N, {st.get('lon', 'N/A')}¬∞E
Pays/R√©gion: Maroc (r√©gion de Safi)
Timestamp: {station_data.get('station_timestamp', 'N/A')}

Mesures actuelles:
- Vitesse du vent: {sd.get('speed_ms', 'N/A')} m/s
- Direction du vent: {sd.get('dir_deg', 'N/A')}¬∞
- Temp√©rature de l'air: {sd.get('air_temp_c', 'N/A')}¬∞C
- Humidit√© relative: {sd.get('rh', 'N/A')}%

Ces donn√©es repr√©sentent les mesures en temps r√©el de la station m√©t√©orologique GP2 situ√©e au Maroc."""
            
            documents.append(LangchainDocument(
                page_content=station_text,
                metadata={
                    "source": "station_gp2_realtime",
                    "data_type": "station_realtime",
                    "timestamp": station_data.get('station_timestamp', ''),
                    "station_name": st.get('name', 'GP2')
                }
            ))
        
        # Document 2: Statistiques pr√©vision √† la position GP2
        if forecast_data and forecast_data.get("available", True):
            sp = forecast_data.get("station_position", {})
            
            # Statistiques de temp√©rature
            if "temp_stats_at_station" in forecast_data:
                stats = forecast_data["temp_stats_at_station"]
                
                stats_text = f"""STATISTIQUES PR√âVISION OPEN-METEO - POSITION STATION GP2

‚ö†Ô∏è IMPORTANT - ZONE G√âOGRAPHIQUE :
Ces pr√©visions concernent UNIQUEMENT la position de la station GP2 au Maroc (r√©gion de Safi).
Ces donn√©es ne couvrent PAS d'autres pays ou r√©gions comme la France, l'Espagne, etc.

Position station: {sp.get('lat', 'N/A')}¬∞N, {sp.get('lon', 'N/A')}¬∞E
Pays/R√©gion: Maroc (r√©gion de Safi)
Mod√®le: {forecast_data.get('model', 'auto')}
Nombre de points de donn√©es: {stats.get('count', 'N/A')}

Temp√©ratures pr√©vues √† la position exacte de la station GP2 (interpol√©es):
- Temp√©rature moyenne: {stats.get('mean', 'N/A'):.2f}¬∞C
- Temp√©rature minimale: {stats.get('min', 'N/A'):.2f}¬∞C
- Temp√©rature maximale: {stats.get('max', 'N/A'):.2f}¬∞C

Ces statistiques sont calcul√©es √† partir des pr√©visions Open-Meteo interpol√©es √† la position exacte de la station GP2 au Maroc."""
                
                documents.append(LangchainDocument(
                    page_content=stats_text,
                    metadata={
                        "source": "forecast_stats_station",
                        "data_type": "forecast_statistics",
                        "model": forecast_data.get('model', 'auto'),
                        "stat_type": "temperature_at_station"
                    }
                ))
            
            # Statistiques d'humidit√© relative
            if "rh_stats_at_station" in forecast_data:
                rh_stats = forecast_data["rh_stats_at_station"]
                
                rh_stats_text = f"""STATISTIQUES HUMIDIT√â RELATIVE PR√âVISION OPEN-METEO - POSITION STATION GP2

‚ö†Ô∏è IMPORTANT - ZONE G√âOGRAPHIQUE :
Ces pr√©visions concernent UNIQUEMENT la position de la station GP2 au Maroc (r√©gion de Safi).

Position station: {sp.get('lat', 'N/A')}¬∞N, {sp.get('lon', 'N/A')}¬∞E
Pays/R√©gion: Maroc (r√©gion de Safi)
Mod√®le: {forecast_data.get('model', 'auto')}
Nombre de points de donn√©es: {rh_stats.get('count', 'N/A')}

Humidit√© relative pr√©vue √† la position exacte de la station GP2 (interpol√©e):
- Humidit√© relative moyenne: {rh_stats.get('mean', 'N/A'):.2f}%
- Humidit√© relative minimale: {rh_stats.get('min', 'N/A'):.2f}%
- Humidit√© relative maximale: {rh_stats.get('max', 'N/A'):.2f}%

Ces statistiques sont calcul√©es √† partir des pr√©visions Open-Meteo interpol√©es √† la position exacte de la station GP2 au Maroc."""
                
                documents.append(LangchainDocument(
                    page_content=rh_stats_text,
                    metadata={
                        "source": "forecast_rh_stats_station",
                        "data_type": "forecast_statistics",
                        "model": forecast_data.get('model', 'auto'),
                        "stat_type": "humidity_at_station"
                    }
                ))
            
            # Document 3: S√©rie temporelle d√©taill√©e (temp√©rature et humidit√©) - TOUTE LA SEMAINE
            if "forecast_timeseries" in forecast_data:
                forecast_series = forecast_data["forecast_timeseries"]
                timeseries_text = "S√âRIE TEMPORELLE PR√âVISION OPEN-METEO - STATION GP2 (TOUTE LA SEMAINE)\n\n"
                timeseries_text += "‚ö†Ô∏è IMPORTANT - ZONE G√âOGRAPHIQUE :\n"
                timeseries_text += "Ces pr√©visions concernent UNIQUEMENT la position de la station GP2 au Maroc (r√©gion de Safi).\n"
                timeseries_text += "Ces donn√©es ne couvrent PAS d'autres pays ou r√©gions.\n\n"
                timeseries_text += f"Mod√®le: {forecast_data.get('model', 'auto')}\n"
                timeseries_text += f"Position station: {forecast_data.get('station_position', {}).get('lat', 'N/A')}¬∞N, {forecast_data.get('station_position', {}).get('lon', 'N/A')}¬∞E\n"
                timeseries_text += "Pays/R√©gion: Maroc (r√©gion de Safi)\n"
                timeseries_text += f"P√©riode couverte: TOUTE LA SEMAINE (7 jours) - {len(forecast_series)} √©ch√©ances disponibles\n\n"
                timeseries_text += "Temp√©ratures et humidit√© relative pr√©vues √† la position GP2 par √©ch√©ance (TOUTE LA SEMAINE):\n\n"
                
                # Afficher toutes les √©ch√©ances de la semaine (pas de limite)
                for fc in forecast_series:
                    h = fc.get("hour", 0)
                    temp_station = fc.get("temp_at_station")
                    temp_mean = fc.get("temp_mean")
                    rh_station = fc.get("rh_at_station")
                    rh_mean = fc.get("rh_mean")
                    
                    if temp_station is not None or rh_station is not None:
                        # Convertir les heures en jours pour plus de clart√©
                        days = h // 24
                        hours_remainder = h % 24
                        if days > 0:
                            time_label = f"J+{days} ({h}h)"
                        else:
                            time_label = f"{h}h"
                        
                        timeseries_text += f"+{time_label}:"
                        if temp_station is not None:
                            timeseries_text += f" Temp: {temp_station:.2f}¬∞C (position GP2)"
                            if temp_mean is not None:
                                timeseries_text += f" | Moyenne grille: {temp_mean:.2f}¬∞C"
                        if rh_station is not None:
                            timeseries_text += f" | Humidit√©: {rh_station:.2f}% (position GP2)"
                            if rh_mean is not None:
                                timeseries_text += f" | Moyenne grille: {rh_mean:.2f}%"
                        timeseries_text += "\n"
                
                timeseries_text += f"\nCes donn√©es repr√©sentent les pr√©visions interpol√©es √† la position exacte de la station GP2 au Maroc pour TOUTE LA SEMAINE ({len(forecast_series)} √©ch√©ances)."
                timeseries_text += "\nLes pr√©visions couvrent 7 jours complets avec des donn√©es de temp√©rature ET d'humidit√© relative √† chaque √©ch√©ance."
                
                documents.append(LangchainDocument(
                    page_content=timeseries_text,
                    metadata={
                        "source": "forecast_timeseries_station",
                        "data_type": "forecast_timeseries",
                        "model": forecast_data.get('model', 'auto'),
                        "hours_count": len(forecast_data["forecast_timeseries"])
                    }
                ))
            
            # Document 4: Statistiques sur toute la grille (temp√©rature et humidit√©)
            grid = forecast_data.get("grid", {})
            grid_text = f"""STATISTIQUES PR√âVISION OPEN-METEO - GRILLE COMPL√àTE

‚ö†Ô∏è IMPORTANT - ZONE G√âOGRAPHIQUE COUVERTE :
Les donn√©es suivantes concernent UNIQUEMENT une petite zone g√©ographique autour de la station GP2 au Maroc (r√©gion de Safi).
Cette zone ne couvre PAS d'autres pays ou r√©gions comme la France, l'Espagne, l'Italie, etc.

Domaine g√©ographique:
- Latitude: {grid.get('lat_min', 'N/A')}¬∞N √† {grid.get('lat_max', 'N/A')}¬∞N
- Longitude: {grid.get('lon_min', 'N/A')}¬∞E √† {grid.get('lon_max', 'N/A')}¬∞E
- R√©solution grille: {grid.get('ny', 'N/A')}x{grid.get('nx', 'N/A')} points
- Mod√®le: {forecast_data.get('model', 'auto')}
- Pays/R√©gion: Maroc (r√©gion de Safi)

Statistiques de temp√©rature sur toute la grille:
"""
            
            if "temp_stats_grid" in forecast_data:
                stats_grid = forecast_data["temp_stats_grid"]
                grid_text += f"""- Temp√©rature moyenne: {stats_grid.get('mean', 'N/A'):.2f}¬∞C
- Temp√©rature minimale: {stats_grid.get('min', 'N/A'):.2f}¬∞C
- Temp√©rature maximale: {stats_grid.get('max', 'N/A'):.2f}¬∞C
"""
            
            if "rh_stats_grid" in forecast_data:
                rh_stats_grid = forecast_data["rh_stats_grid"]
                grid_text += f"""
Statistiques d'humidit√© relative sur toute la grille:
- Humidit√© relative moyenne: {rh_stats_grid.get('mean', 'N/A'):.2f}%
- Humidit√© relative minimale: {rh_stats_grid.get('min', 'N/A'):.2f}%
- Humidit√© relative maximale: {rh_stats_grid.get('max', 'N/A'):.2f}%
"""
            
            grid_text += "\nCes statistiques repr√©sentent les valeurs moyennes, minimales et maximales sur toute la zone g√©ographique couverte par la grille (UNIQUEMENT la r√©gion de Safi, Maroc)."
            
            if "temp_stats_grid" in forecast_data or "rh_stats_grid" in forecast_data:
                documents.append(LangchainDocument(
                    page_content=grid_text,
                    metadata={
                        "source": "forecast_stats_grid",
                        "data_type": "forecast_statistics",
                        "model": forecast_data.get('model', 'auto'),
                        "stat_type": "temperature_and_humidity_grid"
                    }
                ))
        
        # Document 5: Donn√©es globales (si disponibles)
        if global_data and global_data.get("available", False):
            stats = global_data.get("stats", {})
            location = global_data.get("location", "Localit√© inconnue")
            
            global_text = f"""DONN√âES M√âT√âOROLOGIQUES GLOBALES - {location.upper()}

‚ö†Ô∏è IMPORTANT - SOURCE GLOBALE :
Ces donn√©es proviennent de l'API Open-Meteo et concernent {location}.
Ces donn√©es sont diff√©rentes des donn√©es locales de la station GP2 (Maroc).

Position: {global_data.get('lat', 'N/A')}¬∞N, {global_data.get('lon', 'N/A')}¬∞E
Localit√©: {location}
Mod√®le: {global_data.get('model', 'auto')}
Nombre de points de donn√©es: {stats.get('count', 'N/A')}

Statistiques de temp√©rature pour {location} (sur 7 jours):
- Temp√©rature moyenne: {stats.get('mean', 'N/A'):.2f}¬∞C
- Temp√©rature minimale: {stats.get('min', 'N/A'):.2f}¬∞C
- Temp√©rature maximale: {stats.get('max', 'N/A'):.2f}¬∞C

Ces donn√©es repr√©sentent les pr√©visions Open-Meteo pour {location} et sont distinctes des donn√©es de la station GP2 au Maroc."""
            
            documents.append(LangchainDocument(
                page_content=global_text,
                metadata={
                    "source": "forecast_global",
                    "data_type": "forecast_global",
                    "location": location,
                    "model": global_data.get('model', 'auto'),
                    "stat_type": "temperature_global"
                    }
                ))
        
        # Document 6: Analyse spatiale de la grille temps r√©el (donn√©es visualis√©es sur l'interface) - OPTIMIS√â
        if station_data and "temp" in station_data and "grid" in station_data:
            try:
                # Conversion optimis√©e : utiliser directement les listes Python pour les stats simples
                temp_list = station_data["temp"]
                grid_info = station_data["grid"]
                
                # Calculer les statistiques de base sans conversion numpy compl√®te (plus rapide)
                temp_flat = []
                for row in temp_list:
                    if isinstance(row, list):
                        temp_flat.extend([v for v in row if v is not None and not (isinstance(v, float) and np.isnan(v))])
                    else:
                        if row is not None and not (isinstance(row, float) and np.isnan(row)):
                            temp_flat.append(row)
                
                if len(temp_flat) > 0:
                    temp_max_val = float(max(temp_flat))
                    temp_min_val = float(min(temp_flat))
                    temp_mean_val = float(sum(temp_flat) / len(temp_flat))
                    
                    # Calcul simplifi√© des coordonn√©es approximatives (sans conversion compl√®te de la grille)
                    # Approximation : centre de la grille pour les extrema
                    max_lat = (grid_info.get("lat_min", 0) + grid_info.get("lat_max", 0)) / 2
                    max_lon = (grid_info.get("lon_min", 0) + grid_info.get("lon_max", 0)) / 2
                    min_lat = max_lat
                    min_lon = max_lon
                    
                    spatial_text = f"""ANALYSE SPATIALE DE LA GRILLE TEMPS R√âEL - DONN√âES VISUALIS√âES SUR L'INTERFACE

‚ö†Ô∏è IMPORTANT - DONN√âES VISUALIS√âES SUR LA CARTE 2D :
Ces donn√©es correspondent EXACTEMENT √† ce qui est affich√© sur la carte 2D de l'interface.
La grille couvre la zone autour de la station GP2 au Maroc (r√©gion de Safi).
Ces valeurs sont celles que l'utilisateur voit lorsqu'il survole la carte avec sa souris.

‚ö†Ô∏è IMPORTANT - CHAMP FUSIONN√â (OPEN METEO CORRIG√â PAR LA STATION) :
Ces donn√©es ne sont PAS uniquement des donn√©es Open-Meteo brutes.
Ces donn√©es sont le R√âSULTAT DE LA FUSION entre :
1. Les donn√©es Open-Meteo (mod√®le m√©t√©orologique)
2. Les mesures r√©elles de la station GP2

Le processus de fusion corrige les donn√©es Open-Meteo en utilisant les mesures de la station GP2
via une √©quation de Helmholtz, ce qui am√©liore la pr√©cision des donn√©es sur toute la grille.
Les valeurs affich√©es sont donc plus pr√©cises que les donn√©es Open-Meteo brutes car elles
sont calibr√©es avec les mesures r√©elles de la station.

Domaine g√©ographique de la grille affich√©e sur la carte :
- Latitude: {grid_info.get('lat_min', 'N/A')}¬∞N √† {grid_info.get('lat_max', 'N/A')}¬∞N
- Longitude: {grid_info.get('lon_min', 'N/A')}¬∞E √† {grid_info.get('lon_max', 'N/A')}¬∞E
- R√©solution: {grid_info.get('ny', 'N/A')}x{grid_info.get('nx', 'N/A')} points
- Nombre total de points sur la grille: {grid_info.get('ny', 0) * grid_info.get('nx', 0)}

ANALYSE DE TEMP√âRATURE SUR LA GRILLE VISUALIS√âE (CHAMP FUSIONN√â) :
- Temp√©rature moyenne sur toute la grille affich√©e: {temp_mean_val:.2f}¬∞C
- Temp√©rature minimale sur la grille: {temp_min_val:.2f}¬∞C
- Temp√©rature maximale sur la grille: {temp_max_val:.2f}¬∞C
- Amplitude thermique (diff√©rence max-min): {temp_max_val - temp_min_val:.2f}¬∞C

ZONES EXTREMES VISIBLES SUR LA CARTE :
- Zone la plus chaude visible sur la carte: {temp_max_val:.2f}¬∞C √† la position {max_lat:.4f}¬∞N, {max_lon:.4f}¬∞E
- Zone la plus froide visible sur la carte: {temp_min_val:.2f}¬∞C √† la position {min_lat:.4f}¬∞N, {min_lon:.4f}¬∞E

Ces valeurs repr√©sentent les temp√©ratures du CHAMP FUSIONN√â affich√©es sur la carte 2D de l'interface.
L'utilisateur peut voir ces valeurs en survolant la carte avec sa souris."""
                    
                    # Analyse simplifi√©e de l'humidit√© (sans conversion numpy compl√®te)
                    if "rh" in station_data:
                        rh_list = station_data["rh"]
                        rh_flat = []
                        for row in rh_list if isinstance(rh_list, list) else [rh_list]:
                            if isinstance(row, list):
                                rh_flat.extend([v for v in row if v is not None and not (isinstance(v, float) and np.isnan(v))])
                            else:
                                if row is not None and not (isinstance(row, float) and np.isnan(row)):
                                    rh_flat.append(row)
                        
                        if len(rh_flat) > 0:
                            rh_mean = float(sum(rh_flat) / len(rh_flat))
                            rh_min = float(min(rh_flat))
                            rh_max = float(max(rh_flat))
                            spatial_text += f"""

ANALYSE D'HUMIDIT√â RELATIVE SUR LA GRILLE VISUALIS√âE (CHAMP FUSIONN√â) :
- Humidit√© moyenne sur toute la grille affich√©e: {rh_mean:.2f}%
- Humidit√© minimale: {rh_min:.2f}%
- Humidit√© maximale: {rh_max:.2f}%"""
                    
                    # Analyse simplifi√©e du vent (sans conversion numpy compl√®te)
                    if "u" in station_data and "v" in station_data:
                        u_list = station_data["u"]
                        v_list = station_data["v"]
                        speed_flat = []
                        u_rows = u_list if isinstance(u_list, list) else [u_list]
                        v_rows = v_list if isinstance(v_list, list) else [v_list]
                        for u_row, v_row in zip(u_rows, v_rows):
                            u_vals = u_row if isinstance(u_row, list) else [u_row]
                            v_vals = v_row if isinstance(v_row, list) else [v_row]
                            for u_val, v_val in zip(u_vals, v_vals):
                                if u_val is not None and v_val is not None:
                                    if not (isinstance(u_val, float) and np.isnan(u_val)) and not (isinstance(v_val, float) and np.isnan(v_val)):
                                        speed = float((u_val**2 + v_val**2)**0.5)  # Plus rapide que np.sqrt
                                        speed_flat.append(speed)
                        
                        if len(speed_flat) > 0:
                            speed_mean = float(sum(speed_flat) / len(speed_flat))
                            speed_max = float(max(speed_flat))
                            speed_min = float(min(speed_flat))
                            spatial_text += f"""

ANALYSE DU VENT SUR LA GRILLE VISUALIS√âE (CHAMP FUSIONN√â) :
- Vitesse moyenne du vent sur la grille affich√©e: {speed_mean:.2f} m/s ({speed_mean*3.6:.2f} km/h)
- Vitesse minimale: {speed_min:.2f} m/s ({speed_min*3.6:.2f} km/h)
- Vitesse maximale: {speed_max:.2f} m/s ({speed_max*3.6:.2f} km/h)"""
                    
                    documents.append(LangchainDocument(
                        page_content=spatial_text,
                        metadata={
                            "source": "grid_spatial_analysis",
                            "data_type": "spatial_analysis",
                            "grid_type": "realtime",
                            "visualization": "map_2d"
                        }
                    ))
            except Exception as e:
                logger.warning(f"Erreur lors de l'analyse spatiale de la grille: {e}")
        
        # Document 7: Analyse spatiale des pr√©visions (donn√©es visualisables)
        if forecast_data and forecast_data.get("available", True) and "forecast_timeseries" in forecast_data:
            try:
                forecast_series = forecast_data["forecast_timeseries"]
                if forecast_series:
                    all_temps_grid = [f.get("temp_mean") for f in forecast_series if f.get("temp_mean") is not None]
                    all_rh_grid = [f.get("rh_mean") for f in forecast_series if f.get("rh_mean") is not None]
                    
                    forecast_spatial_text = """ANALYSE SPATIALE DES PR√âVISIONS - DONN√âES VISUALISABLES SUR L'INTERFACE

‚ö†Ô∏è IMPORTANT - PR√âVISIONS VISUALISABLES :
Ces donn√©es correspondent aux pr√©visions qui peuvent √™tre affich√©es sur l'interface en s√©lectionnant diff√©rentes √©ch√©ances (0h, 24h, 48h, 72h, 96h, 120h, 144h, 168h).
L'utilisateur peut visualiser ces pr√©visions sur la carte 2D en changeant l'√©ch√©ance.

"""
                    
                    if all_temps_grid:
                        max_temp_overall = max(all_temps_grid)
                        min_temp_overall = min(all_temps_grid)
                        
                        # Trouver l'√©ch√©ance avec la temp√©rature max/min
                        max_hour = None
                        min_hour = None
                        for f in forecast_series:
                            if f.get("temp_mean") == max_temp_overall:
                                max_hour = f.get("hour")
                            if f.get("temp_mean") == min_temp_overall:
                                min_hour = f.get("hour")
                        
                        forecast_spatial_text += f"""ANALYSE TEMPORELLE ET SPATIALE DES PR√âVISIONS - TEMP√âRATURE :
- Temp√©rature moyenne maximale pr√©vue (sur toute la p√©riode): {max_temp_overall:.2f}¬∞C (√† +{max_hour}h)
- Temp√©rature moyenne minimale pr√©vue (sur toute la p√©riode): {min_temp_overall:.2f}¬∞C (√† +{min_hour}h)
- Amplitude thermique pr√©vue: {max_temp_overall - min_temp_overall:.2f}¬∞C

"""
                    
                    if all_rh_grid:
                        max_rh_overall = max(all_rh_grid)
                        min_rh_overall = min(all_rh_grid)
                        
                        # Trouver l'√©ch√©ance avec l'humidit√© max/min
                        max_rh_hour = None
                        min_rh_hour = None
                        for f in forecast_series:
                            if f.get("rh_mean") == max_rh_overall:
                                max_rh_hour = f.get("hour")
                            if f.get("rh_mean") == min_rh_overall:
                                min_rh_hour = f.get("hour")
                        
                        forecast_spatial_text += f"""ANALYSE TEMPORELLE ET SPATIALE DES PR√âVISIONS - HUMIDIT√â RELATIVE :
- Humidit√© relative moyenne maximale pr√©vue (sur toute la p√©riode): {max_rh_overall:.2f}% (√† +{max_rh_hour}h)
- Humidit√© relative moyenne minimale pr√©vue (sur toute la p√©riode): {min_rh_overall:.2f}% (√† +{min_rh_hour}h)
- Amplitude d'humidit√© pr√©vue: {max_rh_overall - min_rh_overall:.2f}%

"""
                    
                    forecast_spatial_text += """Ces pr√©visions peuvent √™tre visualis√©es sur l'interface en s√©lectionnant l'√©ch√©ance correspondante.
L'utilisateur peut voir ces valeurs en survolant la carte avec sa souris apr√®s avoir s√©lectionn√© l'√©ch√©ance."""
                    
                    if all_temps_grid or all_rh_grid:
                        documents.append(LangchainDocument(
                            page_content=forecast_spatial_text,
                            metadata={
                                "source": "forecast_spatial_analysis",
                                "data_type": "spatial_analysis",
                                "grid_type": "forecast",
                                "visualization": "map_2d_forecast"
                            }
                        ))
            except Exception as e:
                logger.warning(f"Erreur lors de l'analyse spatiale des pr√©visions: {e}")
        
        return documents
    
    @staticmethod
    def create_vector_store(documents: List[LangchainDocument], cached_embeddings=None):
        """Cr√©e une base de donn√©es vectorielle FAISS avec embeddings (optimis√© avec cache)"""
        if not documents:
            return None
        
        try:
            # R√©utiliser les embeddings en cache si disponibles
            if cached_embeddings is None:
                embeddings = HuggingFaceEmbeddings(
                    model_name="sentence-transformers/all-mpnet-base-v2"
                )
            else:
                embeddings = cached_embeddings
            
            db = FAISS.from_documents(documents, embeddings)
            logger.info(f"Base vectorielle cr√©√©e avec {len(documents)} documents")
            return db
        except Exception as e:
            logger.error(f"Erreur cr√©ation base vectorielle: {e}")
            return None
    
    @staticmethod
    def keyword_search(db, query: str, k: int = 10) -> List[LangchainDocument]:
        """Recherche par mots-cl√©s bas√©e sur TF-IDF (identique √† llama.py)"""
        all_docs = []
        if hasattr(db, 'docstore'):
            for doc_id in db.index_to_docstore_id.values():
                all_docs.append(db.docstore.search(doc_id))
        else:
            return []
        
        texts = [doc.page_content for doc in all_docs]
        
        vectorizer = TfidfVectorizer(
            stop_words='english',
            max_features=2000,
            ngram_range=(1, 2)
        )
        try:
            tfidf_matrix = vectorizer.fit_transform(texts)
            query_vec = vectorizer.transform([query])
            
            similarities = cosine_similarity(query_vec, tfidf_matrix).flatten()
            sorted_indices = np.argsort(similarities)[::-1]
            
            return [all_docs[i] for i in sorted_indices[:k]]
        except:
            return []
    
    @staticmethod
    def deduplicate_documents(documents: List[LangchainDocument]) -> List[LangchainDocument]:
        """D√©duplique les documents bas√©s sur le contenu"""
        seen_contents = set()
        unique_docs = []
        
        for doc in documents:
            content_hash = hash(doc.page_content[:200])
            if content_hash not in seen_contents:
                seen_contents.add(content_hash)
                unique_docs.append(doc)
        
        return unique_docs
    
    @staticmethod
    def rerank_documents(query: str, documents: List[LangchainDocument]) -> List[LangchainDocument]:
        """Re-rank les documents par pertinence avec CrossEncoder (optimis√© : skip si peu de documents)"""
        # Si peu de documents, skip le re-ranking pour gagner du temps
        if len(documents) <= 3:
            return documents
        
        try:
            cross_encoder = CrossEncoder(RERANKER_MODEL)
            
            pairs = [(query, doc.page_content) for doc in documents]
            scores = cross_encoder.predict(pairs)
            
            scored_docs = list(zip(documents, scores))
            scored_docs.sort(key=lambda x: x[1], reverse=True)
            
            return [doc for doc, score in scored_docs]
        except Exception as e:
            logger.warning(f"Re-ranking non disponible: {str(e)}")
            return documents
    
    @staticmethod
    def hybrid_retrieval(db, query: str, k: int = 10) -> List[LangchainDocument]:
        """Recherche hybride: s√©mantique uniquement (optimis√© pour performance maximale)"""
        if db is None:
            return []
        
        # Recherche s√©mantique uniquement (plus rapide que hybride)
        semantic_results = db.similarity_search(query, k=k)
        
        # Pas de recherche TF-IDF ni de re-ranking pour maximiser la vitesse
        return semantic_results[:k]
    
    @staticmethod
    def validate_retrieval_quality(query: str, retrieved_docs: List[LangchainDocument]) -> List[LangchainDocument]:
        """Filtre les documents non pertinents"""
        relevant_docs = []
        query_keywords = set(query.lower().split())
        
        for doc in retrieved_docs:
            doc_words = set(doc.page_content.lower().split())
            overlap = len(query_keywords & doc_words)
            relevance_score = overlap / len(query_keywords) if query_keywords else 0
            
            if relevance_score > 0.05:
                relevant_docs.append(doc)
        
        return relevant_docs if relevant_docs else retrieved_docs


class WindyChatbot:
    """Chatbot sp√©cialis√© pour r√©pondre aux questions sur les donn√©es m√©t√©o GP2 et Open Meteo avec RAG"""
    
    def __init__(self):
        self.llm = None
        self.vector_store = None
        self.current_conversation_id = None
        self.messages = []
        self._data_hash = None  # Cache pour √©viter de reconstruire la base vectorielle
        self._cached_embeddings = None  # Cache des embeddings
        if CEREBRAS_AVAILABLE:
            try:
                self.llm = ChatCerebras(
                    model="llama3.1-8b",
                    temperature=0.7,
                    max_tokens=TOKEN_LIMITS["max_tokens_per_request"]
                )
            except Exception as e:
                logger.error(f"Erreur initialisation Cerebras: {e}")
                self.llm = None
        
        # Initialiser avec un message de bienvenue
        self.messages = [
            {
                "role": "assistant",
                "content": "Bonjour ! Je suis votre assistant m√©t√©orologique. Posez-moi des questions sur les donn√©es de la station GP2 ou les pr√©visions Open-Meteo."
            }
        ]
    
    def update_data(self, station_data: Dict, forecast_data: Optional[Dict] = None, global_data: Optional[Dict] = None):
        """Met √† jour les donn√©es et reconstruit la base vectorielle seulement si n√©cessaire"""
        # Calculer un hash des donn√©es pour √©viter de reconstruire inutilement
        data_str = json.dumps({
            "station": station_data.get("station_timestamp", "") if station_data else None,
            "forecast": forecast_data.get("hour", "") if forecast_data else None,
            "global": global_data.get("location", "") if global_data else None
        }, sort_keys=True, default=str)
        current_hash = hashlib.md5(data_str.encode()).hexdigest()
        
        # Si les donn√©es n'ont pas chang√©, ne pas reconstruire la base vectorielle
        if current_hash == self._data_hash and self.vector_store is not None:
            logger.debug("Donn√©es inchang√©es, utilisation du cache de la base vectorielle")
            return
        
        # Cr√©er les documents √† partir des donn√©es m√©t√©o (locale + globale)
        documents = WeatherRAGProcessor.create_weather_documents(station_data, forecast_data, global_data)
        
        # Cr√©er/mettre √† jour la base vectorielle
        if documents:
            self.vector_store = WeatherRAGProcessor.create_vector_store(documents, cached_embeddings=self._cached_embeddings)
            # Mettre en cache les embeddings pour la prochaine fois
            if self.vector_store:
                # Les embeddings sont r√©utilis√©s via le param√®tre cached_embeddings
                # On garde une r√©f√©rence pour la prochaine fois
                if self._cached_embeddings is None:
                    # Cr√©er les embeddings une seule fois
                    from langchain_huggingface import HuggingFaceEmbeddings
                    self._cached_embeddings = HuggingFaceEmbeddings(
                        model_name="sentence-transformers/all-mpnet-base-v2"
                    )
            self._data_hash = current_hash
        else:
            self.vector_store = None
            self._data_hash = None
    
    def get_rag_prompt(self, context: str, question: str) -> str:
        """Retourne le prompt RAG optimis√© avec v√©rification g√©ographique"""
        return """Tu es un expert analyste m√©t√©orologique qui doit r√©pondre de mani√®re CLAIRE, SYNTH√âTIQUE et COMPL√àTE.

CONTEXTE FOURNI :
{context}

‚ö†Ô∏è R√àGLE G√âOGRAPHIQUE CRITIQUE :
Les donn√©es disponibles peuvent inclure :
1. Des donn√©es locales de la station GP2 (Maroc, r√©gion de Safi)
2. Des donn√©es globales pour d'autres pays/r√©gions sp√©cifiques (si mentionn√©es dans la question)

‚ö†Ô∏è DONN√âES TEMPS R√âEL - CHAMP FUSIONN√â :
Les donn√©es temps r√©el (affich√©es sur la carte 2D) ne sont PAS uniquement des donn√©es Open-Meteo.
Ce sont des donn√©es FUSIONN√âES qui combinent :
- Les donn√©es Open-Meteo (mod√®le m√©t√©orologique)
- Les mesures r√©elles de la station GP2

Le processus de fusion corrige les donn√©es Open-Meteo en utilisant les mesures de la station GP2
via une √©quation de Helmholtz, ce qui am√©liore la pr√©cision des donn√©es sur toute la grille.
Les valeurs affich√©es sont donc plus pr√©cises que les donn√©es Open-Meteo brutes car elles
sont calibr√©es avec les mesures r√©elles de la station.

‚ö†Ô∏è PR√âVISIONS DE LA SEMAINE - TEMP√âRATURE ET HUMIDIT√â :
Les donn√©es de pr√©vision couvrent TOUTE LA SEMAINE (7 jours) avec des √©ch√©ances √† 0h, 24h, 48h, 72h, 96h, 120h, 144h, 168h.
Chaque √©ch√©ance contient :
- Temp√©rature interpol√©e √† la position exacte de la station GP2
- Temp√©rature moyenne sur toute la grille
- Humidit√© relative interpol√©e √† la position exacte de la station GP2
- Humidit√© relative moyenne sur toute la grille

Tu peux r√©pondre aux questions sur :
- Les pr√©visions de temp√©rature et d'humidit√© pour n'importe quelle √©ch√©ance de la semaine
- Les statistiques (moyenne, min, max) sur toute la semaine
- Les tendances et √©volutions de temp√©rature et d'humidit√© au cours de la semaine
- Les comparaisons entre diff√©rentes √©ch√©ances

‚ö†Ô∏è DONN√âES VISUALIS√âES SUR L'INTERFACE :
Les donn√©es disponibles correspondent EXACTEMENT √† ce qui est affich√© sur la carte 2D et le globe 3D de l'interface.
Tu peux r√©pondre aux questions sur :
- Les valeurs √† n'importe quel point de la grille affich√©e sur la carte (champ fusionn√©)
- Les zones les plus chaudes/froides VISIBLES sur la carte (pas du globe entier)
- Les gradients et variations spatiales sur la zone visualis√©e
- Les comparaisons entre diff√©rentes zones de la grille affich√©e
- Les pr√©visions √† diff√©rentes √©ch√©ances visualisables sur l'interface
- Les zones les plus humides/s√®ches, les plus vent√©es sur la carte affich√©e

IMPORTANT - QUESTIONS GLOBALES :
Si la question concerne une analyse globale/mondiale (comme "zone la plus chaude du globe", "temp√©rature mondiale", etc.),
r√©ponds IMM√âDIATEMENT et CLAIREMENT que ces donn√©es ne sont pas disponibles car elles n√©cessitent une couverture mondiale compl√®te.
NE JAMAIS essayer de r√©pondre avec les donn√©es locales de GP2 pour ce type de questions.

IMPORTANT - QUESTIONS SUR LA ZONE VISUALIS√âE :
Si la question concerne "la zone la plus chaude" ou "la zone la plus froide" SANS mentionner "du globe" ou "mondiale",
alors la question porte probablement sur la zone VISUALIS√âE sur la carte (r√©gion de Safi).
Dans ce cas, utilise les donn√©es d'analyse spatiale de la grille pour r√©pondre avec les coordonn√©es et valeurs exactes.

Si la question mentionne un pays, une r√©gion ou une ville diff√©rente (comme la France, l'Espagne, Paris, etc.), 
utilise les donn√©es globales correspondantes si elles sont disponibles dans le contexte.
Si des donn√©es globales sont pr√©sentes, elles sont clairement identifi√©es avec leur localit√©.
NE JAMAIS utiliser les donn√©es de GP2 (Maroc) pour r√©pondre √† une question sur un autre pays ou r√©gion, 
SAUF si des donn√©es globales pour ce pays/r√©gion sont pr√©sentes dans le contexte.

R√àGLES STRICTES POUR LA R√âPONSE :
1. SYNTH√âTISE : R√©ponds de mani√®re claire et concise, sans r√©p√©titions inutiles
2. COMPL√àTE : Inclus toutes les informations pertinentes du contexte, mais de mani√®re organis√©e
3. STRUCTUR√âE : Utilise des paragraphes courts et des listes √† puces si n√©cessaire pour la clart√©
4. PR√âCISE : Utilise les valeurs num√©riques EXACTES du contexte - ne les modifie pas
5. CONTEXTUELLE : Mentionne la source des donn√©es (station GP2, pr√©visions Open-Meteo) et la zone g√©ographique quand c'est pertinent
6. DIRECTE : Va droit au but, √©vite les phrases trop longues ou les explications superflues
7. Si l'information n'est pas dans le contexte, dis clairement "Cette information n'est pas disponible dans les donn√©es fournies"
8. Si plusieurs sources contiennent des informations compl√©mentaires, int√®gre-les toutes de mani√®re synth√©tique
9. Utilise les statistiques pr√©-calcul√©es (moyennes, min, max) directement - ne les recalcule pas
10. NE TRONQUE JAMAIS ta r√©ponse - fournis une r√©ponse compl√®te m√™me si elle est longue

QUESTION : {question}

R√âPONSE CLAIRE ET SYNTH√âTIQUE BAS√âE SUR LE CONTEXTE (sans troncature) :""".format(context=context, question=question)
    
    def generate_response(self, question: str, station_data: Dict, forecast_data: Optional[Dict] = None) -> str:
        """G√©n√®re une r√©ponse √† une question en utilisant RAG hybride"""
        
        # V√©rifier si c'est une question globale qui n√©cessite des donn√©es mondiales compl√®tes
        # Mais permettre les questions sur "la zone la plus chaude" de la r√©gion visualis√©e
        question_lower = question.lower()
        is_about_globe = any(phrase in question_lower for phrase in [
            "du globe", "du monde", "mondiale", "globale", "plan√©taire", 
            "partout", "tous les pays", "tous les continents", "toute la plan√®te"
        ])
        
        if is_global_question(question) and is_about_globe:
            return "D√©sol√©, je ne peux pas r√©pondre √† cette question car elle n√©cessite des donn√©es m√©t√©orologiques mondiales compl√®tes que je n'ai pas √† ma disposition. Les donn√©es disponibles concernent uniquement la station GP2 (Maroc, r√©gion de Safi) et quelques pays/r√©gions sp√©cifiques sur demande. Pour des questions sur des zones g√©ographiques sp√©cifiques, je peux vous aider si vous mentionnez le pays ou la ville concern√©e."
        
        # D√©tecter si la question concerne un autre pays/r√©gion et r√©cup√©rer les donn√©es globales
        # Si l'API globale √©choue, on continue avec les donn√©es locales
        global_data = None
        try:
            global_data = detect_and_fetch_global_data(question)
        except Exception as e:
            logger.warning(f"Erreur lors de la d√©tection/r√©cup√©ration des donn√©es globales: {e}. Continuation avec les donn√©es locales.")
            global_data = None
        
        # Mettre √† jour les donn√©es et la base vectorielle (locale + globale si disponible)
        try:
            self.update_data(station_data, forecast_data, global_data)
        except Exception as e:
            logger.error(f"Erreur lors de la mise √† jour des donn√©es: {e}", exc_info=True)
            # Essayer sans les donn√©es globales
            try:
                self.update_data(station_data, forecast_data, None)
            except Exception as e2:
                logger.error(f"Erreur critique lors de la mise √† jour des donn√©es: {e2}", exc_info=True)
                return "D√©sol√©, une erreur technique s'est produite lors du traitement de votre question. Veuillez r√©essayer."
        
        # Si pas de base vectorielle, utiliser le mode d√©grad√©
        if self.vector_store is None:
            return self._fallback_response(question, station_data, forecast_data)
        
        # R√©cup√©ration s√©mantique uniquement (optimis√© pour vitesse maximale)
        try:
            retrieved_docs = WeatherRAGProcessor.hybrid_retrieval(self.vector_store, question, k=3)  # R√©duit √† 3 documents
            
            # Validation de la pertinence
            relevant_docs = WeatherRAGProcessor.validate_retrieval_quality(question, retrieved_docs)
            
            if not relevant_docs:
                return "D√©sol√©, je n'ai pas trouv√© d'informations pertinentes dans les donn√©es disponibles pour r√©pondre √† cette question."
            
            # Construire le contexte √† partir des documents r√©cup√©r√©s
            context_parts = []
            for i, doc in enumerate(relevant_docs):
                source_info = f"Source: {doc.metadata.get('source', 'Donn√©es m√©t√©o')}"
                if 'data_type' in doc.metadata:
                    source_info += f" | Type: {doc.metadata['data_type']}"
                context_parts.append(f"[Document {i+1}] {source_info}\n{doc.page_content}")
            
            context = "\n\n".join(context_parts)
            
            # Cr√©er le prompt RAG
            full_prompt = self.get_rag_prompt(context, question)
            
            # G√©n√©rer la r√©ponse avec le LLM en utilisant safe_cerebras_call
            if self.llm:
                try:
                    response_text = safe_cerebras_call(self.llm, full_prompt)
                    
                    # Valider la r√©ponse
                    response_text = self.validate_response(response_text, station_data, forecast_data)
                    return response_text
                except Exception as e:
                    logger.error(f"Erreur appel Cerebras: {e}")
                    return self._fallback_response(question, station_data, forecast_data)
            else:
                return self._fallback_response(question, station_data, forecast_data)
                
        except Exception as e:
            logger.error(f"Erreur RAG: {e}")
            return self._fallback_response(question, station_data, forecast_data)
    
    def validate_response(self, response: str, station_data: Dict, forecast_data: Optional[Dict] = None) -> str:
        """Valide et corrige la r√©ponse pour √©viter les hallucinations"""
        response_lower = response.lower()
        
        # V√©rifier si la r√©ponse mentionne des pr√©visions non disponibles alors qu'elles le sont
        if forecast_data and forecast_data.get("available", True):
            if any(phrase in response_lower for phrase in [
                "pr√©visions ne sont pas disponibles",
                "pr√©visions ne sont pas accessibles",
                "donn√©es de pr√©vision ne sont pas disponibles",
                "open-meteo ne sont pas disponibles"
            ]):
                response = response.replace(
                    "les pr√©visions ne sont pas disponibles",
                    "les pr√©visions sont disponibles"
                ).replace(
                    "pr√©visions ne sont pas disponibles",
                    "pr√©visions sont disponibles"
                )
                logger.warning("Correction appliqu√©e: pr√©visions mentionn√©es comme non disponibles")
        
        # V√©rifier si la r√©ponse mentionne des donn√©es pour un pays/r√©gion diff√©rent
        # mais utilise quand m√™me les donn√©es de GP2 (hallucination g√©ographique)
        other_countries = ["france", "paris", "lyon", "espagne", "spain", "italie", "italy", 
                          "allemagne", "germany", "angleterre", "england", "europe"]
        mentions_other_country = any(country in response_lower for country in other_countries)
        mentions_gp2 = "gp2" in response_lower or "maroc" in response_lower or "safi" in response_lower
        
        if mentions_other_country and mentions_gp2:
            # La r√©ponse m√©lange des pays diff√©rents avec GP2 - c'est suspect
            # V√©rifier si c'est juste pour expliquer la diff√©rence ou si c'est une hallucination
            if not any(phrase in response_lower for phrase in [
                "ne sont pas disponibles", "non disponible", "pas disponible pour",
                "uniquement", "seulement", "concernent uniquement"
            ]):
                logger.warning("R√©ponse suspecte: m√©lange de zones g√©ographiques d√©tect√©")
                # Ne pas modifier automatiquement, mais logger l'avertissement
        
        # Ne pas tronquer la r√©ponse - laisser le LLM g√©n√©rer une r√©ponse compl√®te
        return response
    
    def _fallback_response(self, question: str, station_data: Dict, forecast_data: Optional[Dict] = None) -> str:
        """R√©ponse de secours sans LLM"""
        question_lower = question.lower()
        
        if any(word in question_lower for word in ["vent", "vitesse", "speed"]):
            if station_data and "station_data" in station_data:
                speed = station_data["station_data"].get("speed_ms", "N/A")
                dir_deg = station_data["station_data"].get("dir_deg", "N/A")
                return f"Selon les donn√©es de la station GP2, la vitesse du vent est de {speed} m/s et la direction est de {dir_deg}¬∞."
            return "Les donn√©es de vent ne sont pas disponibles actuellement."
        
        elif any(word in question_lower for word in ["temp√©rature", "temp", "chaud", "froid", "moyenne"]):
            if forecast_data and forecast_data.get("available", True) and "temp_stats_at_station" in forecast_data:
                stats = forecast_data["temp_stats_at_station"]
                return f"Selon les pr√©visions Open-Meteo interpol√©es √† la position de la station GP2, la temp√©rature moyenne est de {stats.get('mean', 'N/A'):.2f}¬∞C, avec un minimum de {stats.get('min', 'N/A'):.2f}¬∞C et un maximum de {stats.get('max', 'N/A'):.2f}¬∞C."
            elif station_data and "station_data" in station_data:
                temp = station_data["station_data"].get("air_temp_c", "N/A")
                return f"La temp√©rature de l'air mesur√©e par la station GP2 est de {temp}¬∞C."
            return "Les donn√©es de temp√©rature ne sont pas disponibles actuellement."
        
        elif any(word in question_lower for word in ["humidit√©", "rh", "humidity"]):
            if forecast_data and forecast_data.get("available", True) and "rh_stats_at_station" in forecast_data:
                rh_stats = forecast_data["rh_stats_at_station"]
                return f"Selon les pr√©visions Open-Meteo interpol√©es √† la position de la station GP2, l'humidit√© relative moyenne est de {rh_stats.get('mean', 'N/A'):.2f}%, avec un minimum de {rh_stats.get('min', 'N/A'):.2f}% et un maximum de {rh_stats.get('max', 'N/A'):.2f}%."
            elif station_data and "station_data" in station_data:
                rh = station_data["station_data"].get("rh", "N/A")
                return f"L'humidit√© relative mesur√©e par la station GP2 est de {rh}%."
            return "Les donn√©es d'humidit√© ne sont pas disponibles actuellement."
        
        else:
            return f"Je peux vous aider √† analyser les donn√©es m√©t√©orologiques. Posez-moi une question plus sp√©cifique sur le vent, la temp√©rature, l'humidit√© ou les pr√©visions."
    
    def add_message(self, role: str, content: str):
        """Ajoute un message √† la conversation"""
        self.messages.append({"role": role, "content": content})
    
    def get_messages(self) -> List[Dict]:
        """Retourne tous les messages de la conversation"""
        return self.messages
    
    def save_conversation(self, title: Optional[str] = None) -> Optional[str]:
        """Sauvegarde la conversation actuelle"""
        if self.current_conversation_id:
            return update_existing_conversation(self.current_conversation_id, self.messages)
        else:
            conv_id = save_conversation(self.messages, title)
            if conv_id:
                self.current_conversation_id = conv_id
            return conv_id
    
    def load_conversation(self, conversation_id: str) -> bool:
        """Charge une conversation"""
        conv_data = load_conversation(conversation_id)
        if conv_data:
            self.messages = conv_data.get("messages", [])
            self.current_conversation_id = conversation_id
            return True
        return False
    
    def new_conversation(self):
        """D√©marre une nouvelle conversation"""
        # Sauvegarder l'ancienne conversation si elle existe
        if self.messages and len(self.messages) > 1:
            self.save_conversation()
        
        self.messages = [
            {
                "role": "assistant",
                "content": "Bonjour ! Je suis votre assistant m√©t√©orologique. Posez-moi des questions sur les donn√©es de la station GP2 ou les pr√©visions Open-Meteo."
            }
        ]
        self.current_conversation_id = None
    
    def get_usage_summary(self) -> Dict[str, Any]:
        """Retourne le r√©sum√© d'utilisation Cerebras"""
        if not CEREBRAS_AVAILABLE:
            return {
                "available": False,
                "message": "Chatbot non disponible - Cl√© API Cerebras manquante"
            }
        return cerebras_token_manager.get_usage_summary()


# Instance globale du chatbot
_chatbot_instance = None

def get_chatbot() -> WindyChatbot:
    """Retourne l'instance singleton du chatbot"""
    global _chatbot_instance
    if _chatbot_instance is None:
        if not CEREBRAS_AVAILABLE:
            logger.warning("‚ö†Ô∏è  Chatbot initialis√© en mode d√©grad√© (cl√© API manquante)")
        _chatbot_instance = WindyChatbot()
    return _chatbot_instance
